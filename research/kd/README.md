# Knowledge Distillation
## Introduction
This module holds several PyTorch re-implementation of knowledge distillation algorithm.

## Reference
1. Hinton G, Vinyals O, Dean J. [Distilling the knowledge in a neural network](https://arxiv.org/pdf/1503.02531) [J]. arXiv preprint arXiv:1503.02531, 2015.
2. Yuan L, Tay F E H, Li G, et al. [Revisiting Knowledge Distillation via Label Smoothing Regularization](http://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf) [C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3903-3911.
3. Cho J H, Hariharan B. [On the efficacy of knowledge distillation](http://openaccess.thecvf.com/content_ICCV_2019/papers/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.pdf) [C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 4794-4802.